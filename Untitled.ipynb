{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0374dd94-9dc7-4043-bb6f-d0714bab6f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd8c6c89-6972-4c3e-8906-b37b19326225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98441ccf-2c80-4db8-bc4e-6b03f2d14df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "804accb8-9a1f-434a-b022-23a821856276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhrit\\Research\\sp25\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "041986f2-da19-4e6b-a3f9-7531e82945b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9edc6f35-a9d3-40f7-9d8e-afdf00bfdd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5959ef81-9e71-4cf2-9848-005e6f0add88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-large\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd8e1784-5a43-4617-aede-78600903623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ChangeDetectionDataset(Dataset):\n",
    "    def __init__(self, root_dir: str, tokenizer, max_length: int = 512):\n",
    "        \"\"\"\n",
    "        root_dir should be e.g. \"easy/train\" or \"hard/validation\"\n",
    "        Expects files: problem-*.txt and truth-problem-*.json\n",
    "        \"\"\"\n",
    "        self.examples = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # find all txt files\n",
    "        for txt_path in glob(os.path.join(root_dir, \"problem-*.txt\")):\n",
    "            base = os.path.splitext(os.path.basename(txt_path))[0]  # e.g. \"problem-3\"\n",
    "            json_path = os.path.join(root_dir, f\"truth-{base}.json\")\n",
    "            if not os.path.exists(json_path):\n",
    "                continue\n",
    "\n",
    "            # read sentences\n",
    "            with open(txt_path, encoding=\"utf-8\") as f:\n",
    "                lines = [l.strip() for l in f.readlines() if l.strip()]\n",
    "            # read labels\n",
    "            with open(json_path, encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            changes: List[int] = data[\"changes\"]\n",
    "\n",
    "            # build pairs (sent_i, sent_{i+1})\n",
    "            for i, label in enumerate(changes):\n",
    "                if i + 1 < len(lines):\n",
    "                    self.examples.append({\n",
    "                        \"sent1\": lines[i],\n",
    "                        \"sent2\": lines[i+1],\n",
    "                        \"label\": label,\n",
    "                    })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.examples[idx]\n",
    "        # tokenizer will handle truncation and padding (padding done in collator)\n",
    "        enc = self.tokenizer(\n",
    "            ex[\"sent1\"],\n",
    "            ex[\"sent2\"],\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            # return_tensors=\"pt\", #???\n",
    "        )\n",
    "        enc[\"labels\"] = torch.tensor(ex[\"label\"], dtype=torch.long)\n",
    "        return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90185760-5cc1-44e0-ac45-27cfdde8f2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ChangeDetectionDataset(\"hard/train\", tokenizer)\n",
    "eval_ds  = ChangeDetectionDataset(\"hard/validation\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44141fd0-307c-4c6a-9839-7e46d1c4b671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(train_ds)):\n",
    "#     assert min(train_ds[i]['input_ids']) >= 0\n",
    "#     assert min(train_ds[i]['attention_mask']) in [0,1]\n",
    "#     assert min(train_ds[i]['input_ids']) <= 50265\n",
    "#     assert train_ds[i]['labels'] in [0,1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c97a7322-ac4c-4bce-8a6e-5ad29ec2c661",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEncoderClassifier(nn.Module):\n",
    "    def __init__(self, model_name=\"vinai/bertweet-large\", num_labels=2, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.bertweet = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.bertweet.config.hidden_size\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bertweet(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # take [CLS] token\n",
    "        cls_rep = outputs.last_hidden_state[:, 0, :]\n",
    "        cls_rep = self.dropout(cls_rep)\n",
    "        logits = self.classifier(cls_rep)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss, logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "325142c9-6f8e-4700-92fb-9c6867b223d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"vinai/bertweet-large\",\n",
    "    num_labels=2\n",
    ")\n",
    "model.to('cuda')\n",
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2407e56f-a80f-433a-b006-5fd6a7950a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98e1676a-a415-43bc-b2b7-5ade748f74e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=data_collator,\n",
    "                     # mixed precision to save memory & boost throughput\n",
    "      # load data in parallel\n",
    ")\n",
    "print(\"Trainer device:\", trainer.model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490dd54d-5035-425c-985b-d4fb09d63040",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "504f7066-09c2-4547-8c9a-c51d20d5d0d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11848b2e-49c2-498a-bdc9-bdbf6e40ccb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
